#!/bin/bash -
# SPDX-FileCopyrightText: <text> Â© 2016, 2017, 2019, 2020 Alan D. Salewski <ads@salewski.email> </text>
# SPDX-License-Identifier: GPL-2.0-or-later
#
#     This program is free software; you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation; either version 2 of the License, or
#     (at your option) any later version.
#
#     This program is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program; if not, write to the Free Software Foundation,
#     Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301,, USA.

# ads-github-fetch-all-upstreams: Make an authenticated call to the GitHub API
# (using curl) to obtain a list of all of my repositories. For each, if a
# local directory (beneath the current location) is found that matches the
# repo name (and that directory is a git working directory, and it has a
# remote named 'upstream' defined), then this program will change into the
# local working directory for that repo and run a 'git fetch upstream'
# command.
#
# All requests are made over HTTPS.
#
# Authentication
# ==============
# No authentication data is used directly; the curl(1) '-n' (--netrc) option
# is used, so the user is expected to have his GitHub credentials stored in a
# ~/.netrc file. For details on setting that up, see curl(1) and
# netrc(5). Note that curl will not use the ~/.netrc file if the permissions
# allow reading by group or other.
#
# HINT: The relevant line ~/.netrc file content should have the form:
#
#     machine api.github.com login YOUR_USER_NAME password YOUR_GITHUB_PERSONAL_ACCESS_TOKEN
#
# CAVEAT: Use of the netrc(5) configuration means that this tool is ultimately
# using HTTP Basic authentication (username/password) with the GitHub
# API. Though all communication is conducted over HTTPS (so is encrypted "on
# the wire"), this authentication mechanism (available through 2020-11-13; see
# below) allows this tool to access the full power of the GitHub API.
#
# UPDATE (2020-10-05): GitHub has officially deprecated password-based
#        authentication, and it will be disabled entirely on 2020-11-13:
#
#            https://developer.github.com/changes/2020-02-14-deprecating-password-auth/
#
#        A better approach (which you were probably using already, anyway) is
#        to create what GitHub calls a "personal access token":
#
#            https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token
#
#        You use a "personal access token" in basically the same way that you
#        would use a password (you put it in your ~/.netrc file as the value
#        for the 'password' field), but the token itself can be configured to
#        have more constrained permissions (you decide what it can do at the
#        time you create the token).
#
#        You may wish to create a token that is not able to perform certain
#        actions (e.g., deleting repositories). That will have a minor impact
#        on the ads-github-tools, but they will simply tell you when they are
#        not able to perform some action.
#
# DO NOT USE THIS TOOL UNLESS YOU TRUST THAT IT IS NOT ABUSING THIS TRUST.
#
#
# Motivation
# ==========
# With even a relatively small number of GitHub repositories, keeping local
# trees up to date with upstreams can be a chore. This tool performs one piece
# of automation that can contribute to a solution.
#
# TODO
# ====
#
#     When this program was initially introduced, the author scribbled notes
#     in this comments section to remind himself about bugs to be fixed and
#     potential new features that might be useful. Both are now tracked in the
#     project's GitHub "issues" page here:
#
#         https://github.com/salewski/ads-github-tools/issues
#
#
# See Also:
# =========
#
#     * The 'Repos' section of the GitHub API (v3)
#       https://developer.github.com/v3/repos/
#
#     * The 'git-hub' tool:
#       https://github.com/sociomantic-tsunami/git-hub

declare -r PROG='ads-github-fetch-all-upstreams'

set -o pipefail

declare -r COPYRIGHT_DATES='2016, 2017, 2019, 2020'

# declare -r MAINTAINER='@DFLT_MAINTAINER_FULL@'
declare -r MAINTAINER='@PACKAGE_BUGREPORT@'  # value filtered-in at build time

declare -r VERSION='@VERSION@'  # value filtered-in at build time

declare -r gl_const_build_date='@BUILD_DATE@'  # value filtered-in at build time
declare -r gl_const_release="${VERSION}  (built: ${gl_const_build_date})"
# declare -r gl_const_release="${VERSION}"


# Note that we use an "application" $TRACING flag separate from bash's
# built-in 'xtrace' (set -x) shell option. This allows us to have a general
# notion of user-requested verbosity separate from any selectively placed
# 'set -x/set +x' sections we may plug into the code while working on the
# program itself; similarly for places where we might want to avoid trace
# output while debugging by selectively placing 'set +x/set -x' sections; both
# of those behaviors would be much more cumbersome to achieve if we just
# checked for 'x' in $- at runtime.
#
BE_VERBOSE=false   # enable with one '-v' (--verbose) opt
DEBUGGING=false    # enable with two '-v' (--verbose) opts
TRACING=false      # enable with three or more '-v' (--verbose) opts

RE_ALL_DIGITS='^[[:digit:]]+$'


# GitHub issue #28: Unset GREP_OPTIONS to make grep(1) behavior predictable.
#
# This only has an effect if the grep program in use is GNU grep; other grep
# implementations did not recognize or alter their behavior based on the
# 'GREP_OPTIONS' variable. GNU grep prior to version 2.11 (~2014-11) would
# append values from this variable to the command line, which would make
# behavior of our invocations in the current program unpredictable. Versions
# of GNU grep 2.11 or newer no longer behave that way, but do emit a warning
# on stderr about the change in behavior.
#
# Unsetting GREP_OPTIONS here has the effect of making our grep invocations
# predictable (when older versions of GNU grep are in use) and also of
# suppressing the spurious warning:
#
#     grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
#
# when newer versions of GNU grep are in use.
#
# See: https://github.com/salewski/ads-github-tools/issues/28
#
unset GREP_OPTIONS


# If specified, the '--clone-if-missing' command line option directs this
# program to clone any repository from GitHub where the local git working
# directory is missing. The default behavior (when that option is not
# specified) is to just operate on the working directories that are present.
#
# Note that we invoke the 'git-hub' tool (via 'git hub clone') to implement
# this feature. That means that we'll not only clone the repository if it is
# missing, but we'll also have the 'upstream' remote set up (if the repo is a
# fork of another repo). This works regardless of whether or not the working
# directory was present when this command was invoked.
#
DO_CLONE_IF_NOT_PRESENT=false


# The default behavior of the program is to operate only on working
# directories that are present ("operate only on these" semantics).
#
# Sometimes you want exactly the opposite behavior, though ("fetch all the new
# stuff" semantics). Our '--missing-only' command line option allows the user
# to request that behavior.
#
# See also: https://github.com/salewski/ads-github-tools/issues/22
#
FETCH_ONLY_MISSING=false


# It may be the case that we have cloned one of our repositories manually, but
# for whatever reason have not set up the 'upstream' remote to point to the
# parent GitHub repository from which ours was cloned.
#
# By default, this program will not perform any action on any such working
# directories. But if the '-u' ('--upstream-remote-if-missing') option is
# specified, we will take action to setup a git remote named "upstream" using
# the parent repo's clone url.
#
DO_CREATE_UPSTREAM_REMOTE_IF_NOT_PRESENT=false


# Folks using GitHub Enterprise might access their API at a different
# location, so we parameterize the base URL for the GitHub API. MAKE SURE THIS
# URL INDICATES HTTPS (not just HTTP -- you do not want your HTTP Basic Auth
# credentials being transmitted in cleartext!).
#
# This is NOT declared read-only here because there is some minor fixup that
# we attempt so users do not need to be concerned with whether or not the URL
# ends with a slash. For our purposes we need the value to end with a slash,
# and will add a slash to the end if it is not specified here.
#
declare    gl_const_github_api_base_url='https://api.github.com/'

# We'll help future-proof this program by explicitly requesting version 3 of
# the GitHub API (although it is the default at the time of writing
# (2016-04)).
#
declare -r gl_const_http_accept_github_version='Accept: application/vnd.github.v3+json'


# By default we'll use the external programs found at configure-time (values
# are filtered-in here at build time). But we allow the user to override any
# particular tool by setting an environment variable named after the tool
# (with hyphen chars changed to underscores).

# jq - command line JSON parser and manipulation language
#      see: https://github.com/stedolan/jq
#
JQ_PROG="${JQ:-@JQ_PROG@}"
RM_PROG="${RM:-@RM@}"
TR_PROG="${TR:-@TR_PROG@}"
WC_PROG="${WC:-@WC_PROG@}"

CAT_PROG="${CAT:-@CAT@}"
GIT_PROG="${GIT:-@GIT_PROG@}"
SED_PROG="${SED:-@SED@}"

CURL_PROG="${CURL:-@CURL_PROG@}"
ECHO_PROG="${ECHO:-@ECHO_PROG@}"
HEAD_PROG="${HEAD:-@HEAD_PROG@}"
TAIL_PROG="${TAIL:-@TAIL_PROG@}"

XARGS_PROG="${XARGS:-@XARGS_PROG@}"

MKTEMP_PROG="${MKTEMP:-@MKTEMP_PROG@}"

# FIXME: How to make this selectable? Would need to influence git's search
#        path to find the specified version of the tool, but how to do that
#        without changing PATH?
#
# git-hub - command line GitHub API tool; works as 'git' subcommand if found
#           on PATH
#           see: https://github.com/sociomantic-tsunami/git-hub
#                https://www.kernel.org/pub/software/scm/git/docs/howto/new-command.html
#
# GIT_HUB_PROG="${GIT_HUB:-@GIT_HUB_PROG@}"
GIT_HUB_PROG='git-hub'

declare -a NEEDED_EXTERNAL_PROGS=(
    "${RM_PROG}"
    "${JQ_PROG}"
    "${TR_PROG}"
    "${WC_PROG}"

    "${CAT_PROG}"
    "${GIT_PROG}"
    "${SED_PROG}"

    "${CURL_PROG}"
    "${ECHO_PROG}"
    "${HEAD_PROG}"
    "${TAIL_PROG}"

    "${XARGS_PROG}"

    "${MKTEMP_PROG}"

    "${GIT_HUB_PROG}"
)

for ext_tool in "${NEEDED_EXTERNAL_PROGS[@]}"; do

    t_path=$(builtin type -p "${ext_tool}")
    if test $? -ne 0; then
        printf "${PROG} (error): was unable to locate \"%s\" on PATH; bailing out\n" "${ext_tool}" 1>&2
        exit 1
    fi

    if $DEBUGGING; then
        printf "${PROG} (debug): path to external tool \"%s\": %s\n" "${ext_tool}" "${t_path}" 1>&2
    fi
done


declare -a F_CLEANUP_HOOK_NAMES=()

function f_add_cleanup_hook_name () {
    F_CLEANUP_HOOK_NAMES+=( $1 );
}


function f_cleanup () {

    if test ${#F_CLEANUP_HOOK_NAMES[@]} -eq 0; then
        # No cleanup hooks, so nothing to do
        return
    fi

    local cleanup_hook
    local idx

    let idx=${#F_CLEANUP_HOOK_NAMES[@]}-1

    # Note that we're running the cleanup hooks in opposite order from which
    # they were installed.
    #
    while test $idx -ge 0; do

        cleanup_hook=${F_CLEANUP_HOOK_NAMES[$idx]}

        if $BE_VERBOSE; then
            printf "${PROG} (info): running cleanup hook: [%s]\n" "${cleanup_hook}" 1>&2
        fi

        test -n "$cleanup_hook" && eval "$cleanup_hook"

        let idx=$idx-1
    done
}

function f_cleanup_and_die () {
    f_cleanup
    exit 1
}

trap 'printf "$PROG (warn): HUP signal caught; bailing out\n"  1>&2; f_cleanup_and_die' HUP
trap 'printf "$PROG (warn): INT signal caught; bailing out\n"  1>&2; f_cleanup_and_die' INT
trap 'printf "$PROG (warn): QUIT signal caught; bailing out\n" 1>&2; f_cleanup_and_die' QUIT
trap 'printf "$PROG (warn): TERM signal caught; bailing out\n" 1>&2; f_cleanup_and_die' TERM

trap 'f_cleanup' EXIT



f_print_help () {

    "${CAT_PROG}" <<EOF
usage: $PROG [OPTION...] [REPO...]
Invokes 'git fetch upstream' for all of the user's GitHub repos that are
present. With appropriate options, will clone some or all missing repos and
set up the 'upstream' remote.

Mandatory arguments to long options are mandatory for short options too.

  -h, --help                        Print this help message on stdout
  -V, --version                     Print the version of the program on stdout
  -c, --clone-if-missing            Use git-hub(1) to clone your missing repos from GitHub
  -m, --missing-only                Operate on (clone) only repos that are missing locally (implies '-c')
  -u, --upstream-remote-if-missing  Setup 'upstream' remote in existing repo working dirs, if missing

  -v, --verbose                     Print program progress messages on stderr. Specify multiple
                                      times to increase verbosity: info, debug, and tracing (set -x)

      --                            Signals the end of options and disables further options processing.

Report bugs to $MAINTAINER.
EOF
}

f_print_version () {
    "${CAT_PROG}" <<EOF
${PROG} ${gl_const_release}
Copyright (C) ${COPYRIGHT_DATES} Alan D. Salewski
License GPLv2+: GNU GPL version 2 or later <http://gnu.org/licenses/gpl.html>.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Written by Alan D. Salewski.
EOF
}


# @param FPATH -- (required) The path to a file to examine.
#
function f_file_ends_with_newline() {

    local __required_count=1
    if test $# -ne ${__required_count}; then
        printf "${PROG} (BUG): ${FUNCNAME}() invoked with %d args; exactly %d required; bailing out\n" \
            $# ${__required_count} 1>&2
        exit 1
    fi

    local __t_fpath=$1

    local __t_line_cnt

    # Feed last character of the file (if any) to wc(1) and see if it counts
    # it as a line. If so, then the file ends with a newline; otherwise it
    # does not.
    #
    # This technique from:
    #     https://stackoverflow.com/a/25749716
    #
    __t_line_cnt=$("${TAIL_PROG}" -c1 "${__t_fpath}" | "${WC_PROG}" -l)
    if test $? -ne 0; then
        printf "${PROG} (error): was unable to obtain line count from file (%s); bailing out\n" \
               "${__t_fpath}" 1>&2
        exit 1
    fi
    if test -z "${__t_line_cnt}"; then
        printf "${PROG} (error): tail/wc succeeded, but line count for file (%s) is empty?; bailing out\n" \
               "${__t_fpath}" 1>&2
        exit 1
    fi
    if [[ "${__t_line_cnt}" =~ $RE_ALL_DIGITS ]]; then :; else
        printf "${PROG} (error): got non-numeric line count (%s) for file (%s); bailing out\n" \
               "${__t_line_cnt}" \
               "${__t_fpath}" 1>&2
        exit 1

    fi

    if test ${__t_line_cnt} -gt 0; then
        return 0  # true
    fi
    return 1  # false
}


# Invoked by our accumulating '-v' (--verbose) option. Increases the program
# output verbosity level by "one stage".
#
# If we have not yet increased the verbosity, then enables info-level output
# ($BE_VERBOSE).
#
# If we are currently at info-level verbosity, then enables debug-level output
# ($DEBUGGING).
#
# If we are currently at debug-level verbosity, then enables trace-level
# output ($TRACING, set -x).
#
# If we are already at trace-level verbosity, then this function has no effect
# (is effectively a NOOP).
#
f_maybe_increase_verbosity () {

    if $BE_VERBOSE; then

        if $DEBUGGING; then

            # We've seen at least two -v opt before, so now we're turning
            # tracing on.

            if $TRACING; then
                : $PROG \(trace: $LINENO\): tracing already enabled
            else
                # See comments at the decl spot for $TRACING pertaining to why
                # we maintain an "application" $TRACING flag separate from
                # bash's built-in 'xtrace' (set -x) shell option.

                # Enable tracking before setting our app-level $TRACING flag
                # so that setting it is the first thing that appears in the
                # trace.
                set -x
                TRACING=true
            fi
        else
            # Second -v opt we're seeing
            DEBUGGING=true
            # Just to give a warm and fuzzy...
            printf "${PROG} (debug): debug-level output enabled\n" 1>&2
        fi
    else
        # First -v opt we're seeing
        BE_VERBOSE=true
    fi

    return 0  # success
}


# Common global options for use in (nearly) every 'curl' invocation. Specific
# invocations will require additional options, but such usages should not
# modify the MY_CURL_DEFAULT_OPTS array.
#
# The MY_CURL_OPTS array is provided to be reused per invocation:
#
#     MY_CURL_OPTS=()  # (re)set
#     MY_CURL_OPTS+=( "${MY_CURL_DEFAULT_OPTS[@]}" )  # copy
#     MY_CURL_OPTS+=( YOUR_STUFF )
#
# HTTP response status code gets written to stderr.
#
# The HTTP response payload (if any) is written to the file path named in
# $MY_TMP_CURL_OUT_FPATH.
#
# CAREFUL: This assumes localized, sequential usage patterns of curl(1) and
#          handling of the response body contents. If you need long-term
#          access to the HTTP response body contents, copy it from
#          $MY_TMP_CURL_OUT_FPATH to a different file location so you want
#          have to worry about the data you need getting overwritten by other
#          sections of the code making API requests with curl(1) as configured
#          here.
#
declare -a MY_CURL_DEFAULT_OPTS=()
declare -a MY_CURL_OPTS=()

f_initialize_curl_default_opts () {

    # sanity check
    if test -z "$MY_TMP_CURL_OUT_FPATH"; then
        printf "${PROG} (error): ${FUNCNAME}(): MY_TMP_CURL_OUT_FPATH is not set; bailing out\n" 1>&2
        exit 1
    fi

    # This disables output of curl's progress meter /and/ output of error messages...
    MY_CURL_DEFAULT_OPTS+=( '--silent' )
    # ...but this re-enables output of the error messages.
    MY_CURL_DEFAULT_OPTS+=( '--show-error' )


    # Force use of TLS 1.2 (or later). Writing in 2020, all previous versions
    # are known to be broken and susceptible to known attacks. Note that the
    # '--tlsv1.2' option was added in curl 7.34.0
    #
    # This is absolutely essential since we're using HTTP Basic Auth (see below).
    #
    MY_CURL_DEFAULT_OPTS+=( '--tlsv1.2' )

    # Allow ONLY https, for both the initial request and for redirects
    MY_CURL_DEFAULT_OPTS+=( '--proto')
    MY_CURL_DEFAULT_OPTS+=( 'https')
    MY_CURL_DEFAULT_OPTS+=( '--proto-redir')
    MY_CURL_DEFAULT_OPTS+=( 'https')


    # Tell curl to use HTTP Basic Authentication. This is the curl default, but
    # we're explicit about what we expect (and want to avoid any surprises from
    # weirdo ~/.curlrc files).
    #
    # See also: RFC 7617 "The 'Basic' HTTP Authentication Scheme" (2015-09)
    #
    MY_CURL_DEFAULT_OPTS+=( '--basic' )


    # User's authentication credentials will be obtained from the user's ~/.netrc
    # file. See curl(1) and netrc(5)
    #
    MY_CURL_DEFAULT_OPTS+=( '--netrc'  )

    MY_CURL_DEFAULT_OPTS+=( '--user-agent' )
    MY_CURL_DEFAULT_OPTS+=( "$PROG"        )


    # Even when we're just making HEAD requests, have curl fail
    # MY_CURL_DEFAULT_OPTS+=( '--fail' )


    # Tell the GitHub service that we're trying to speak v3 of the API. Writing in
    # 2020, v3 is the default, but there is also a 'GraphQL API v4' version which
    # we do not use here. Some newer version may become the default in the future,
    # so we are explicit about which version we are using.
    #
    MY_CURL_DEFAULT_OPTS+=( '--header' )
    MY_CURL_DEFAULT_OPTS+=( "${gl_const_http_accept_github_version}" )

    # We always write the (JSON) output to a file in our temporary directory...
    #
    MY_CURL_DEFAULT_OPTS+=( '--output' )
    MY_CURL_DEFAULT_OPTS+=( "${MY_TMP_CURL_OUT_FPATH}" )

    # ...and write the HTTP response status to stdout. This allows for robust
    # error handling. Also, we cannot really know how to interpret the output
    # returned from the remote server until we have examined (at least) the HTTP
    # response code.
    #
    MY_CURL_DEFAULT_OPTS+=( '--write-out'  )
    MY_CURL_DEFAULT_OPTS+=( '%{http_code}' )

    return 0
}


# Makes a HTTP HEAD request to the GitHub service to obtain pagination
# information for pulling all /user/repos data in the minimal number of
# service calls (that is, within the service restriction of 100 repos per
# response "page").
#
# Upon success, emits on stdout the number of "pages" that need to be
# requested.
#
# Recall that the first page is one, not zero.
#
# @param OUT_VAR_NAME -- (required) the name of the global variable to which
#                        the total repos count value should be written.
#
# @return 0 always, if it returns at all. All internal errors are fatal.
#
f_get_gh_user_repos_pagination_count_or_die () {

    local __required_count=1
    if test $# -ne ${__required_count}; then
        printf "${PROG} (BUG): ${FUNCNAME}() invoked with %d args; exactly %d required; bailing out\n" \
            $# ${__required_count} 1>&2
        exit 1
    fi

    local __t_out_var_name=$1

    local __t_http_code
    local __t_estat

    local t_head_response
    local t_http_head_stat_and_label
    local t_http_head_stat
    local t_http_head_label
    local t_http_head_link
    local t_last_page_number

    # Pagination: 
    #
    # HTTP/1.1 200 OK
    # ...
    # Link: <https://api.github.com/user/repos?per_page=100&page=2>; rel="next", <https://api.github.com/user/repos?per_page=100&page=4>; rel="last"

    MY_CURL_OPTS=()  # (re)set
    MY_CURL_OPTS+=( "${MY_CURL_DEFAULT_OPTS[@]}" )  # copy

    MY_CURL_OPTS+=( '--head' )

    __t_http_code=$( "${CURL_PROG}" "${MY_CURL_OPTS[@]}"     \
                                    "${MY_GITHUB_REPOS_URL}" )
    __t_estat=$?

    if test ${__t_estat} -ne 0; then
        printf "${PROG} (error): was error while invoking curl(1) to retrieve repos pagination; bailing out\n" 1>&2
        exit 1
    fi
    if test -z "${__t_http_code}"; then
        printf "${PROG} (error): ${FUNCNAME}(): curl(1) command succeeded, but no HTTP response code emitted; bailing out\n" 1>&2
        exit 1
    fi

    case ${__t_http_code} in

        '200')
            if $DEBUGGING; then
                printf "${PROG} (debug): HEAD request for repos pagination data succeeded\n" 1>&2
            fi
            ;;

        *)
            printf "${PROG} (error): HEAD request for repos pagination data failed; HTTP response code was: \"%s\"; expected 200 (\"OK\"); bailing out\n" \
                   "${__t_http_code}" 1>&2
            exit 1
            ;;
    esac

    if test -s "${MY_TMP_CURL_OUT_FPATH}"; then :; else
        printf "${PROG} (error): HEAD request for repos pagination data succeeded, by response body is empty; bailing out\n" 1>&2
        exit 1
    fi

    t_head_response=$("${TR_PROG}" -d '\r' < "${MY_TMP_CURL_OUT_FPATH}")
    if test $? -ne 0; then
        printf "${PROG} (error): was unable to read HEAD response body for repos pagination data from temporary file (%s); bailing out\n" \
               "${MY_TMP_CURL_OUT_FPATH}" 1>&2
        exit 1
    fi

    # From RFC 7230:
    #
    #     3.1.2.  Status Line
    #
    #        The first line of a response message is the status-line, consisting
    #        of the protocol version, a space (SP), the status code, another
    #        space, a possibly empty textual phrase describing the status code,
    #        and ending with CRLF.
    #
    #          status-line = HTTP-version SP status-code SP reason-phrase CRLF
    #
    #        The status-code element is a 3-digit integer code describing the
    #        result of the server's attempt to understand and satisfy the client's
    #        corresponding request.  The rest of the response message is to be
    #        interpreted in light of the semantics defined for that status code.
    #        See Section 6 of [RFC7231] for information about the semantics of
    #        status codes, including the classes of status code (indicated by the
    #        first digit), the status codes defined by this specification,
    #        considerations for the definition of new status codes, and the IANA
    #        registry.
    #
    #          status-code    = 3DIGIT
    #
    #        The reason-phrase element exists for the sole purpose of providing a
    #        textual description associated with the numeric status code, mostly
    #        out of deference to earlier Internet application protocols that were
    #        more frequently used with interactive text clients.  A client SHOULD
    #        ignore the reason-phrase content.
    #
    #          reason-phrase  = *( HTAB / SP / VCHAR / obs-text )
    #
    # Note that we're following Postel's Prescription to an extent in being
    # lenient about whether or not there is more than a single whitespace
    # token between fields.
    #
    t_http_head_stat_and_label=$(echo "${t_head_response}" | "${HEAD_PROG}" -n 1 | "${SED_PROG}" -n -e '/^HTTP[/]/ { s#^HTTP/[^[:space:]]\{1,\}[[:space:]]\{1,\}\([[:digit:]]\{3\}[[:space:]]\{1,\}.*\)#\1#p;q }')
    if test -z "${t_http_head_stat_and_label}"; then
        printf "${PROG} (error): was unable to extract HTTP status code from responses; bailing out\n    Full HTTP response headers:\n%s\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    if $DEBUGGING; then
        printf "${PROG} (debug): HTTP status and label for pagination info: %s\n" "${t_http_head_stat_and_label}" 1>&2
    fi

    t_http_head_stat=$(  echo "${t_http_head_stat_and_label}" | "${SED_PROG}" -e 's#^\([[:digit:]]\{3\}\)[[:space:]]\{1,\}.*#\1#')
    t_http_head_label=$( echo "${t_http_head_stat_and_label}" | "${SED_PROG}" -e 's#^[[:digit:]]\{3\}[[:space:]]\{1,\}\([[:space:][:alnum:][:punct:]]*\)$#\1#')

    if $DEBUGGING; then
        printf "${PROG} (debug): HTTP status for pagination info: %s\n" "${t_http_head_stat}"  1>&2
        printf "${PROG} (debug): HTTP label for pagination info: %s\n"  "${t_http_head_label}" 1>&2
    fi

    if test -z "${t_http_head_stat}"; then
        printf "${PROG} (error): was unable to determine HTTP status code of pagination info request for repo; bailing out\n" 1>&2
        exit 1
    fi

    # Slice off leading zeros in bogon HTTP response code. Belt AND suspenders...
    #
    t_http_head_stat=$((10#$t_http_head_stat))

    if test $t_http_head_stat -lt 100; then
        # We'll include the full response line to help in troubleshooting. In
        # the worst case scenario we will learn that we mis-parsed it somehow,
        # which is a really a win in the long term.
        printf "${PROG} (error): received bogon HTTP status line with response code less than 100 (\"%s\"); bailing out\n" "${t_http_head_stat_and_label}" 1>&2
        exit 1
    fi

    # Oh hell, if we're gonna check the lower bound, then we're boning it if
    # we don't also check the upper bound...
    if test $t_http_head_stat -gt 599; then
        printf "${PROG} (error): received bogon HTTP status line with response code greater than 599 (\"%s\"); bailing out\n" "${t_http_head_stat_and_label}" 1>&2
        exit 1
    fi

    # So we know that the HTTP status code is somewhere between 100 and 599...

    if test $t_http_head_stat -ge 100 \
    && test $t_http_head_stat -lt 200; then
        printf "${PROG} (error): program does not directly grok HTTP 1xx \"Informational\" status codes; bailing out\n    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    if test $t_http_head_stat -ge 300 \
    && test $t_http_head_stat -lt 400; then
        printf "${PROG} (error): program does not directly grok HTTP 3xx \"Redirection\" status codes; bailing out\n"\
"    If you encounter this error, please submit a bug report; real-world need is likely to get it fixed.\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    if test $t_http_head_stat -ge 400 \
    && test $t_http_head_stat -lt 500; then
        printf "${PROG} (error): program received a HTTP 4xx \"Client Error\" status codes; bailing out\n"\
"    If you encounter this error, please submit a bug report; it suggests there is a problem with the\n"\
"    way in which the program is formulating requests to the upstream GitHub service.\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    if test $t_http_head_stat -ge 500 \
    && test $t_http_head_stat -lt 600; then
        printf "${PROG} (error): program received a HTTP 5xx \"Server Error\" status codes; bailing out\n"\
"    If you encounter this error, it suggests there is a problem with the upstream GitHub service;\n"\
"    please wait a while and try your request later.\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    # Internal sanity check
    if test $t_http_head_stat -ge 200 \
    && test $t_http_head_stat -lt 300; then :; else
        printf "${PROG} (bug): internal check failed. Expected that we received a 2xx \"Successful\" HTTP status code, but really have: \"%s\"; bailing out.\n"\
"    If you encounter this error, there is a serious bug with this program; please submit a bug report.\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    # So we know that the HTTP status code is somewhere between 200 and 299;
    # we're good (probably).

    # FIXME: This will break if the 'Link:' header content spans multiple
    #        lines (which is legit; see RFC 2616 section 4.2 "Message
    #        Headers").

    # Let's find the 'Link: ' header...
    t_http_head_link=$(echo "${t_head_response}" | grep '^Link:[[:space:]]')
    if test $? -ne 0; then

        # It's not necessarily an error if the 'Link:' header is not
        # present. That will happen for resources that do not require any
        # pagination. So we'll examine the individual exit status codes from
        # our pipeline; if grep exited with status 1, it means the header was
        # not found. If it exited with 2 it indicates an error (such as a
        # malformed regex).
        #
        if test "${PIPESTATUS[0]}" -eq 0 \
        && test "${PIPESTATUS[1]}" -eq 1; then
            # We'll emit 1 as the count of pages needed
            printf -v "${__t_out_var_name}" '1'
            return 0  # success
        fi

        # Hopefully grep emitted something helpful on stderr already if we're
        # falling through here...

        printf "${PROG} (error): was error while attempting to locate HTTP \"Link:\" header in HEAD response; no pagination data available; bailing out\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    # After the 'Link: ' header label, we expect the content to contain a
    # comma-space-separated list of values in the form:
    #
    #     <url>; rel="foo", <url>; rel="bar"
    #
    # Example:
    #
    #     Link: <https://api.github.com/user/repos?per_page=100&page=2>; rel="next", <https://api.github.com/user/repos?per_page=100&page=4>; rel="last"
    #
    # We'll look for the rel="last" entry, which should be present even when
    # it's URL value would be the same as the rel="next" entry. The first
    # 'sed' invocation lifts out the URL from the rel="last" entry, and the
    # second then extracts the number of the last page from the 'page=N'
    # parameter.
    #
    t_last_page_number=$( echo "${t_http_head_link}" \
                          | "${SED_PROG}" -n -e 's#.*\([<][^>]\{1,\}[>]\)[[:space:]]*[;][^,]*rel="last"#\1#p' \
                          | "${SED_PROG}" -n -e 's#.*[?&]page=\([[:digit:]]\{1,\}\)[^[:digit:]]#\1#p' )
    if test $? -ne 0; then
        printf "${PROG} (error): was error while attempting to parse last page number from HTTP \"Link:\" header in HEAD response; bailing out\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi
    if test -z "${t_last_page_number}"; then
        printf "${PROG} (error): extracted empty value for last page number from HTTP \"Link:\" header in HEAD response; bailing out\n"\
"    Full HTTP response headers:\n%n\n" \
               "$(echo "${t_head_response}" | "${SED_PROG}" -e 's/^/        /')" 1>&2
        exit 1
    fi

    printf -v "${__t_out_var_name}" '%s' "${t_last_page_number}"
    return 0  # success
}


# @param REPO_PAGES_COUNT -- (required) The number of "repo data" pages
#                            (starting from page 1) to be consume.
#
# @param OUT_VAR_NAME -- (required) the name of the pre-existing global
#                        variable to which the repo data lines should be
#                        appended.
#
# @return 0 always, if it returns at all. All internal errors are fatal.
#
#
# @see f_get_gh_user_repos_pagination_count_or_die()
#
f_build_repo_data_lines_or_die () {

    local __required_count=2
    if test $# -ne ${__required_count}; then
        printf "${PROG} (BUG): ${FUNCNAME}() invoked with %d args; exactly %d required; bailing out\n" \
            $# ${__required_count} 1>&2
        exit 1
    fi

    local __t_repo_pages_cnt=$1
    local __t_out_var_name=$2

    # sanity check
    if test -z "$MY_TMP_DIR"; then
        printf "${PROG} (error): ${FUNCNAME}(): MY_TMP_DIR is not set; bailing out\n" 1>&2
        exit 1
    fi

    local __t_cur_page=0
    local __t_keep_going=true

    local __t_gh_url_for_page

    # We will gather each "page" of data from the GitHub API and use it to
    # produce a partial data structure. Each page's worth of data is stored in
    # a separate temporary file.
    #
    # Once we have the individual partial data structures (one for each of the
    # pages), we will catenate them together to produce the value we write to
    # the output variable.
    local -a __t_repo_data_page_fpaths=()

    local __t_one_tmpfile_fpath
    local __t_tmpfile_catenated_repo_data_fpath

    local __t_http_code
    local __t_estat

    local __t_rsp_body

    while ${__t_keep_going}; do

        let __t_cur_page=${__t_cur_page}+1

        # Our curl(1) command will save the HTTP response to a temporary file
        # that gets reused with each of our curl invocations. So after each
        # successful call, we extract the data we need from that temporary
        # file and write the results to a different temporary file whose file
        # path we generate here.
        #
        __t_one_tmpfile_fpath=$("${MKTEMP_PROG}" --tmpdir="${MY_TMP_DIR}" 'repo-data-pg-'"${__t_cur_page}"'.XXXXXXXXXXXX')
        if test $? -ne 0; then
            printf "${PROG} (error): was unable to generate temporary file name for repo data page %s; bailing out\n" \
                   "${__t_cur_page}" 1>&2
            exit 1
        fi

        __t_gh_url_for_page=${MY_GITHUB_REPOS_URL}'&page='${__t_cur_page}

        if $BE_VERBOSE; then
            printf "${PROG} (info): requesting /user/repos page %s of %s\n" "${__t_cur_page}" "${MY_TOTAL_REPO_PAGES}" 1>&2
        fi

        MY_CURL_OPTS=()  # (re)set
        MY_CURL_OPTS+=( "${MY_CURL_DEFAULT_OPTS[@]}" )  # copy

        MY_CURL_OPTS+=( '--get' )

        __t_http_code=$( "${CURL_PROG}" "${MY_CURL_OPTS[@]}" \
                                        "${__t_gh_url_for_page}" )
        __t_estat=$?
        if test ${__t_estat} -ne 0; then
            printf "${PROG} (error): was error while invoking curl(1) to retrieve repo data page %s; bailing out\n" \
                   "${__t_cur_page}" 1>&2
            exit 1
        fi
        if test -z "${__t_http_code}"; then
            printf "${PROG} (error): ${FUNCNAME}(): curl(1) command succeeded, but no HTTP response code emitted; bailing out\n" 1>&2
            exit 1
        fi

        case ${__t_http_code} in
            '200')
                if $DEBUGGING; then
                    printf "${PROG} (debug): GET request for repo page %s data succeeded\n" "${__t_cur_page}" 1>&2
                fi
                ;;

            *)
                printf "${PROG} (error): GET request for repo page %s data failed; HTTP response code was: \"%s\"; expected 200 (\"OK\"); bailing out\n" \
                       "${__t_cur_page}" \
                       "${__t_http_code}" 1>&2
                exit 1
                ;;
        esac

        if test -s "${MY_TMP_CURL_OUT_FPATH}"; then :; else
            printf "${PROG} (error): GET request for repo page %s data succeeded, by response body is empty; bailing out\n" \
                   "${__t_cur_page}" 1>&2
            exit 1
        fi

        # Add the file to our list before we try to put anything into it
        # because with the following command the shell will create it as an
        # empty file even if the jq(1) command fails.
        #
        __t_repo_data_page_fpaths+=( "${__t_one_tmpfile_fpath}" )

        "${JQ_PROG}" --raw-output '.[] '\
'| select( .owner.type == "User" ) '\
'| .name + ":" + .full_name + ":" + .default_branch + ":" + if (.fork) then "true" else "false" end' \
                     < "${MY_TMP_CURL_OUT_FPATH}" \
                     > "${__t_one_tmpfile_fpath}"
        if test $? -ne 0; then
            printf "${PROG} (error): Was unable to extract data for repo page %s; bailing out\n" \
                   "${__t_cur_page}" 1>&2
            exit 1
        fi
        if test -s "${__t_one_tmpfile_fpath}"; then :; else
            printf "${PROG} (error): Extracted data file for repo page %s is empty; bailing out\n" \
                   "${__t_cur_page}" 1>&2
            exit 1
        fi

        f_file_ends_with_newline "${__t_one_tmpfile_fpath}"
        if test $? -eq 0; then :; else
            if $DEBUGGING; then
                printf "${PROG} (debug): Adding missing newline to temporary file: %s\n" \
                       "${__t_one_tmpfile_fpath}" 1>&2
            fi
            printf '\n' >> "${__t_one_tmpfile_fpath}"
        fi

        # internal sanity check
        f_file_ends_with_newline "${__t_one_tmpfile_fpath}"
        if test $? -eq 0; then :; else
            printf "${PROG} (error): Newline is /still/ missing at end of temporary file: %s; bug?; bailing out\n" \
                   "${__t_one_tmpfile_fpath}" 1>&2
            exit 1
        fi

        if test ${__t_cur_page} -ge ${__t_repo_pages_cnt}; then
            __t_keep_going=false
        fi
    done

    # Catenate the individual repo data files into a single file.
    #
    # We remove each file as we go so we do not end up consuming nearly 200%
    # of the disk space we actually need. Here we care about space over time.
    #
    # XXX: Yes, this is a slow way to do it, but it is far from the slowest
    #      part of this program.
    #
    __t_tmpfile_catenated_repo_data_fpath=$("${MKTEMP_PROG}" --tmpdir="${MY_TMP_DIR}" 'repo-data-cat.XXXXXXXXXXXX')
    if test $? -ne 0; then
        printf "${PROG} (error): was unable to generate temporary file name for catenated repo data pages; bailing out\n" 1>&2
        exit 1
    fi
    > "${__t_tmpfile_catenated_repo_data_fpath}"  # touch
    if test $? -ne 0; then
        printf "${PROG} (error): was unable to create temporary file for catenated repo data pages; bailing out\n" 1>&2
        exit 1
    fi
    for ff in "${__t_repo_data_page_fpaths[@]}"; do
        "${CAT_PROG}" "${ff}" >> "${__t_tmpfile_catenated_repo_data_fpath}"
        if test $? -ne 0; then
            printf "${PROG} (error): was unable to append temporary file (%s) to catenated tmp file (%s); bailing out\n" \
                   "${ff}" \
                   "${__t_tmpfile_catenated_repo_data_fpath}" 1>&2
            exit 1
        fi
        "${RM_PROG}" "${ff}"
        if test $? -ne 0; then
            printf "${PROG} (warning): was unable to delete temporary file: %s; continuing anyway\n" \
                   "${ff}" 1>&2
            # keep going...
        fi
    done


    # For now we'll just slurp up all of the repo info into memory; we'll
    # revisit this if needed, but <famous-last-words>we're not expecting to
    # have enough repos that it will be a problem</famous-last-words>.
    #
    #     $ jq -r '.[] | .name + ":" + .full_name + ":" + .default_branch + ":" + if (.fork) then "true" else "false" end' < all-repos-per_page-105-001.js | head -n 5
    #     aas:salewski/aas:master:true
    #     abcl:salewski/abcl:master:true
    #     abstract-tables:salewski/abstract-tables:master:true
    #     ac-nrepl:salewski/ac-nrepl:master:true
    #     ack2:salewski/ack2:dev:true
    #
    # UPDATE (2016-07-06): [issue #12] Note that we are working only with
    # repos with owner type 'User' (we are ignoring those with owner type
    # 'Organization'). This is to work around the previously unrealized issue
    # that the /user/repos/ data can include repos that that user (as a member
    # of a particular organization) merely has access to, regardless of
    # whether he also has a personal fork of those repos. The authenticated
    # user's personal fork of such a repo, if he has one, is presented
    # separately.
    #
    # XXX: This has the undesirable effect of preventing this program from
    #      working directly on organization-owned repos. The author needs to
    #      further review how those are intended to work, but if this turns
    #      out to be an artificial limitation is interested in having it
    #      fixed. Patches/pull requests welcome!
    #
    # See also: https://github.com/salewski/ads-github-tools/issues/12
    #
    printf -v "${__t_out_var_name}" '%s' \
           "$("${CAT_PROG}" "${__t_tmpfile_catenated_repo_data_fpath}")"
    if test $? -ne 0; then
        printf "${PROG} (error): was unable slurp repo data file into output variable; bailing out\n" 1>&2
        exit 1
    fi

    # cleanup
    "${RM_PROG}" "${__t_tmpfile_catenated_repo_data_fpath}"
    if test $? -ne 0; then
        printf "${PROG} (warning): was unable to delete temporary file: %s; continuing anyway\n" \
               "${__t_tmpfile_catenated_repo_data_fpath}" 1>&2
        # keep going...
    fi

    return 0  # success
}


pos_last_plus_one=$(( $# + 1 ))

# Each value is one or zero, which indicates whether or not the option is
# expected to have an argument.
#
declare -A longopt_spec=(
    ['help']=0      # -h
    ['version']=0   # -V

    ['clone-if-missing']=0            # -c
    ['missing-only']=0                # -m
    ['upstream-remote-if-missing']=0  # -u
    ['verbose']=0                     # -v
)

# internal sanity check
for one_key in "${!longopt_spec[@]}"; do
    one_val=${longopt_spec[${one_key}]}
    if [[ $one_val =~ ^[01]$ ]]; then :; else
        printf "${PROG} (BUG) [line $LINENO]: value (%s) for longopt key '%s' must be either 0 or 1; bailing out\n" \
               "${one_val}" "${one_key}" 1>&2
        exit 1
    fi
done

if test $# -gt 0; then

    # Using getopts in "silent mode". Note that adding '-' to the optstring allows us to
    # process GNU-style long-form options; that option is specified to take an argument to
    # cause getopts to place whatever follows the second '-' character into OPTARG.
    #
    # Note that getopts will automatically stop processing options upon encountering
    # '--', but we still need to deal with the pathological form --=BLAH (no option name,
    # just a value using the equals-sign syntax).
    #
    while getopts ':-:hcmuVv' opt
    do
        : $PROG \(trace: $LINENO\): opt is: $opt

        if test "${opt}" = '-'; then

            # Intercepting processing of long-form option. This conditional
            # block will set up the 'opt', 'OPTARG', and 'OPTIND' variables for
            # the code that follows, just as if getopts had the capability to
            # process long-form options.

            # OPTARG here is one of:
            #
            #     =BLAH    (which means user specified '--=BLAH')
            # or:
            #     foo
            # or:
            #     foo=FOOVAL

            if [[ ${OPTARG} =~ .*=.* ]]; then

                : $PROG \(trace: $LINENO\): OPTARG is name=value style

                # Keep everything up to the first '=' sign. Note that if the
                # option was specified as: --foo=FOOVAL, then $opt here will be
                # 'foo' (no hyphen chars).
                opt=${OPTARG/=*/}
                : $PROG \(trace: $LINENO\): opt is: $opt

                : $PROG \(trace: $LINENO\): a long option name must be at least two characters in length
                if test ${#opt} -le 1; then
                    printf "${PROG} (error): invalid long option '%s'; bailing out\n" "${opt}" 1>&2
                    f_print_help 1>&2
                    exit 1
                fi

                t_exists=false
                for one_key in "${!longopt_spec[@]}"; do
                    if test "${opt}" = "${one_key}"; then
                        t_exists=true
                        break
                    fi
                done

                : $PROG \(trace: $LINENO\): a long option name must be one that the program is expecting
                if $t_exists; then :; else

                    printf "${PROG} (error): invalid long option '%s'; bailing out\n" "${opt}" 1>&2
                    f_print_help 1>&2
                    exit 1
                fi

                # Since we know the option was specified in --foo=BAR form, the
                # option was specified erroneously unless the option's long-form
                # spec indicates that it can accept an argument.
                #
                if test ${longopt_spec[${opt}]} -ne 1; then
                    printf "${PROG} (error): option '%s' does not take an argument; bailing out\n" "${opt}" 1>&2
                    f_print_help 1>&2
                    exit 1
                fi

                OPTARG=${OPTARG#*=}  # keep everything after the first '=' sign
                : $PROG \(trace: $LINENO\): OPTARG is: $OPTARG

            else
                : $PROG \(trace: $LINENO\): OPTARG is name-only style

                opt="$OPTARG"
                : $PROG \(trace: $LINENO\): opt is: $opt

                if test -z "${opt}"; then

                    # This should be a "can't happen" scenario; since bash's 'getopts'
                    # implementation should directly handle the magic '--' token, we
                    # should never fall through here.

                    printf "${PROG} (BUG) [line $LINENO]: received empty OPTARG, which means getopts did not handle the stand-alone '--' token; bailing out\n" 1>&2
                    exit 1
                fi

                : $PROG \(trace: $LINENO\): a non-empty long option name must be at least two characters in length
                if test ${#opt} -lt 2; then
                    printf "${PROG} (error): invalid long option '%s'; bailing out\n" "${opt}" 1>&2
                    f_print_help 1>&2
                    exit 1
                fi

                t_exists=false
                for one_key in "${!longopt_spec[@]}"; do
                    if test "${opt}" = "${one_key}"; then
                        t_exists=true
                        break
                    fi
                done

                : $PROG \(trace: $LINENO\): a long option name must be one that the program is expecting
                if $t_exists; then :; else

                    printf "${PROG} (error): invalid long option '%s'; bailing out\n" "${opt}" 1>&2
                    f_print_help 1>&2
                    exit 1
                fi

                # We know the option was specified in one of the following forms:
                #
                #     --foo
                # or:
                #     --foo FOOVAL
                #
                # The option's long-form spec will tell us whether or not an argument is
                # expected for the option.
                #
                if test ${longopt_spec[${opt}]} -eq 1; then

                    # If bumping OPTIND would put us more than one beyond the "last pos
                    # plus one", then there is no argument provided at position OPTIND for
                    # us to consume.
                    #
                    if (( $(( $OPTIND + 1 )) > pos_last_plus_one )); then

                        printf "${PROG} (error): missing argument for option -${OPTARG}\n" 1>&2
                        f_print_help 1>&2
                        exit 1
                    fi

                    OPTARG=${@:${OPTIND}:1}
                    (( ++OPTIND ))
                    : $PROG \(trace: $LINENO\): manually incremented OPTIND to: $OPTIND
                fi
            fi
        fi

        # Normal getopts style processing happens beneath here, with the slight
        # twist that 'opt' may contain a long-form option name.

        case $opt in

            'h' | 'help' )
                # print help message
                f_print_help
                exit 0
                ;;

            'V' | 'version' )
                # print program version info
                f_print_version
                exit 0
                ;;


            'c' | 'clone-if-missing' )
                DO_CLONE_IF_NOT_PRESENT=true
                ;;

            'm' | 'missing-only')
                FETCH_ONLY_MISSING=true
                # The '-m' option implies the '-c' option, so:
                DO_CLONE_IF_NOT_PRESENT=true
                ;;

            'u' | 'upstream-remote-if-missing' )
                DO_CREATE_UPSTREAM_REMOTE_IF_NOT_PRESENT=true
                ;;


            'v' | 'verbose' )
                # Accumulating 'verbose' opt. A single -v opt simply turns
                # BE_VERBOSE on (info level output); two '-v' opts turns on
                # $DEBUGGING (debug level output); three or more '-v' opts turns
                # tracing on. Note that if you intend to turn tracing on, you'll
                # probably want your -v opts to be the first opts on the command
                # line (so they take effect earlier).
                #
                f_maybe_increase_verbosity
                ;;


            ':')  # getopts put : in opt
                  # Note that we need to restore the leading '-' that getopts
                  # has sliced off.
                  printf "${PROG} (error): missing argument for option -${OPTARG}\n" 1>&2
                  f_print_help 1>&2
                  exit 1
                  ;;

            '?')  # getopts put ? in opt
                  # Unrecognized option. Note that we need to restore the
                  # leading '-' that getopts has sliced off.
                  printf "${PROG} (error): unrecognized option '-%s'; bailing out\n" "${OPTARG}" 1>&2
                  f_print_help 1>&2
                  exit 1
                  ;;

            * )   printf "${PROG} (BUG) [line $LINENO]: unhandled option case; opt: '$opt',  OPTARG: '$OPTARG'\n" 1>&2
                  ;;

        esac
    done
fi

# shift off all arguments already handled
let ii=1;  # shell OPTIND index starts at 1
while (( ii < ${OPTIND} )); do
    shift
    (( ++ii ))
    : $PROG \(trace: $LINENO\): ii is now: $ii
done


# GitHub issue #1: If there are any explicitly requested repos that we do not
# process, then we need to let the user know about it.
#
# See: https://github.com/salewski/ads-github-tools/issues/1
declare -a PROCESSED_REPOS=()

declare -a EXPLICITLY_REQUESTED_REPOS=()

# Any remaining arguments are interpreted as the names of repositories that
# the user specifically wants to operate on.
#
# XXX: We do not currently do anything special to account for the same repo
#      being specified more than once; I'm not yet convinced that we
#      should. If the user tells us to operate on repos "foo bar foo", we'll
#      operate on "foo" twice (though the second will effectively be an
#      elaborate NOOP).
#
while test $# -gt 0 ; do

    _one_repo_name=$1; shift

    if $BE_VERBOSE; then
        printf "${PROG} (info): noting user-specified explicit repo name: \"%s\"\n" "${_one_repo_name}" 1>&2
    fi

    EXPLICITLY_REQUESTED_REPOS+=( "${_one_repo_name}" )
done


MY_TMP_DIR=$("${MKTEMP_PROG}" -t --directory "${PROG}.XXXXXXXX")
if test $? -ne 0; then
    printf "${PROG} (error) was unable to create temporary directory; bailing out\n" 1>&2
    exit 1
fi
#
# This should be a "can't happen" scenario, but since we do a 'rm -fr ...' on
# the value when we are done, we want to be belt-and-suspenders about it...
#
if test -z "${MY_TMP_DIR}"; then
    printf "${PROG} (error) temporary directory path is empty; bailing out\n" 1>&2
    exit 1
fi
function f_cleanup_rmfr_tmpdir () {
    local -a t_rm_opts=()
    t_rm_opts+=('-f')
    t_rm_opts+=('-r')
    if $TRACING; then
        t_rm_opts+=('-v')  # verbose
    fi
    "${RM_PROG}" "${t_rm_opts[@]}" "${MY_TMP_DIR}"
    # Ignore exit status -- Is a cleanup hook, so do not exit the process if
    # 'rm' failed; keep going...
}
F_CLEANUP_HOOK_NAMES+=( 'f_cleanup_rmfr_tmpdir' )

declare -r MY_TMP_CURL_OUT_FPATH="${MY_TMP_DIR}/curl.out"


# Refuse to run if we do not recognize the URL as something that will be
# encrypted on the wire. This is intended to help prevent accidentally
# transmitting HTTP Basic Auth credentials in cleartext.
#
re_starts_with_https='^https://'
if test -z "${gl_const_github_api_base_url}"; then
    printf "${PROG} (error): GitHub API base URL is not defined; bailing out\n" 1>&2
    exit 1
fi
if [[ "${gl_const_github_api_base_url}" =~ $re_starts_with_https ]]; then :; else
    printf "${PROG} (error): configured GitHub API base URL (\"%s\") does not start with 'https://'; bailing out\n" "${gl_const_github_api_base_url}" 1>&2
    exit 1
fi
#
re_ends_with_slash='.*[/]$'
if [[ "${gl_const_github_api_base_url}" =~ $re_ends_with_slash ]]; then :; else
    if $BE_VERBOSE; then
        printf "${PROG} (info): configured GitHub API base URL (\"%s\") does not end with a slash; appending slash character\n" "${gl_const_github_api_base_url}" 1>&2
    fi
    gl_const_github_api_base_url="${gl_const_github_api_base_url}/"
    declare -r gl_const_github_api_base_url
    if $BE_VERBOSE; then
        printf "${PROG} (info): configured GitHub API base URL (modified) is now: \"%s\"\n" "${gl_const_github_api_base_url}" 1>&2
    fi
fi
if $BE_VERBOSE; then
    printf "${PROG} (info): configured GitHub API base URL: \"%s\"\n" "${gl_const_github_api_base_url}" 1>&2
fi


re_starts_with_accept='Accept:[[:space:]]'
if test -z "${gl_const_http_accept_github_version}"; then
    printf "${PROG} (error): HTTP 'Accept:' header for GitHub API version is not defined; bailing out\n" 1>&2
    exit 1
fi
if [[ "${gl_const_http_accept_github_version}" =~ $re_starts_with_accept ]]; then :; else
    printf "${PROG} (error): configured HTTP 'Accept:' header (\"%s\") for GitHub API version does not start with 'Accept: '; bailing out\n" "${gl_const_http_accept_github_version}" 1>&2
    exit 1
fi
if $BE_VERBOSE; then
    printf "${PROG} (info): configured HTTP header for GitHub API version: \"%s\"\n" "${gl_const_http_accept_github_version}" 1>&2
fi


f_initialize_curl_default_opts   # initializes the $MY_CURL_DEFAULT_OPTS array


# Note that the gl_const_http_accept_github_version value is known to always
# end with a slash character.
#
# per_page: The GitHub v3 API allows for retrieving results in "page sizes" up
# to 100 (in this case, 100 repositories). If a 'per_page' value greater than
# 100 is specified, then the service silently behaves as if 100 were
# specified. Since we need to slurp down all of the data for our purposes
# here, we specify the maximum page size so we have to make the fewest number
# of remote calls while paging through the results.
#
MY_GITHUB_REPOS_URL="${gl_const_github_api_base_url}user/repos?per_page=100"


# We'll build up this multi-line value in memory below. Each line will have
# the form:
#
#     repo_name:repo_full_name:default_branch:is_fork
#
# Example of 4 such lines:
#
#     aas:salewski/aas:master:true
#     abcl:salewski/abcl:master:true
#     abstract-tables:salewski/abstract-tables:master:true
#     ac-nrepl:salewski/ac-nrepl:master:true
#
# The 'is_fork' field will always be either 'true' or 'false', and indicates
# whether or not the repo was forked from another GitHub repository. Where the
# value is 'true', callers know they can make a request to the GitHub API's
# /repos/:owner:repo endpoint to request more comprehensive information about
# the repo that would include a 'parent' object. We return the value here
# because it is also useful to know that such a call need not be made if
# you're only interested in the parent data, or just want to know whether or
# not a parent exists.
#
REPO_DATA_LINES=''

# Pagination: 

# HTTP/1.1 200 OK
# ...
# Link: <https://api.github.com/user/repos?per_page=100&page=2>; rel="next", <https://api.github.com/user/repos?per_page=100&page=4>; rel="last"

# Note that we still obtain info on /all/ repos even if the user has specified
# that we operate only on specific repos (in $EXPLICITLY_REQUESTED_REPOS). If
# the user specified only a couple of repos, then we lose here because we
# technically could query the GitHub service for the info on just those
# specific repos to accomplish our work.  If the user explicitly specifies
# more than a few, though, we're back on solid ground because we can typically
# get the info on all available user repos with fewer than 5 GitHub API calls
# (it is unlikely that a typical GitHub user has more than 500 repos). In any
# event, we do not know which page of the results a particular repo will be
# on, so we need to pull full summary data anyway.
#
f_get_gh_user_repos_pagination_count_or_die 'MY_TOTAL_REPO_PAGES'

# Build REPO_DATA_LINES. Each record (line) in the table data structure will
# be in the form:
#
#     <REPO_NAME>:<REPO_FULL_NAME>:<DEFAULT_BRANCH>:<IS_FORK>
#
# Example:
#
#     aas:salewski/aas:master:true
#     abcl:salewski/abcl:master:true
#     abstract-tables:salewski/abstract-tables:master:true
#     ac-nrepl:salewski/ac-nrepl:master:true
#     ack2:salewski/ack2:dev:true
#     ...
f_build_repo_data_lines_or_die "${MY_TOTAL_REPO_PAGES}" 'REPO_DATA_LINES'


while IFS=':' read -r repo_name repo_full_name default_branch is_fork; do

    : $PROG \(trace: $LINENO\): repo_name:      "$repo_name"
    : $PROG \(trace: $LINENO\): repo_full_name: "$repo_full_name"
    : $PROG \(trace: $LINENO\): default_branch: "$default_branch"
    : $PROG \(trace: $LINENO\): is_fork:        "$is_fork"

    # Sanity checking
    if test -z "${repo_name}"; then
        printf "${PROG} (BUG): 'repo_name' value is empty; bailing out\n" 1>&2
        exit 1
    fi
    if test -z "${repo_full_name}"; then
        printf "${PROG} (BUG): 'repo_full_name' value is empty; bailing out\n" 1>&2
        exit 1
    fi
    if test -z "${default_branch}"; then
        printf "${PROG} (BUG): 'default_branch' value is empty; bailing out\n" 1>&2
        exit 1
    fi
    if test -z "${is_fork}"; then
        printf "${PROG} (BUG): 'is_fork' value is empty; bailing out\n" 1>&2
        exit 1
    fi
    if   test "${is_fork}" = 'true';  then :
    elif test "${is_fork}" = 'false'; then :
    else
        printf "${PROG} (BUG): invalid value detected for 'is_fork' (\"%s\"); should be either 'true' or 'false'; bailing out\n" \
               "${is_fork}" 1>&2
        exit 1
    fi

    if test "${#EXPLICITLY_REQUESTED_REPOS[@]}" -gt 0; then

        t_repo_is_keeper=false

        for expl_repo_name in "${EXPLICITLY_REQUESTED_REPOS[@]}"; do

            if test "${expl_repo_name}" = "${repo_name}"; then
                t_repo_is_keeper=true
                break;
            fi
        done

        if $t_repo_is_keeper; then :; else

            if $DEBUGGING; then
                printf "${PROG} (debug): repo \"%s\" is not one of those explicitly requested for operation; skipping (okay)\n" \
                       "${repo_name}" 1>&2
            fi
            continue
        fi
    fi

    if $FETCH_ONLY_MISSING; then

        if test -d "${repo_name}"; then
            # We'll treat the presence of a subdirectory with the repo name as "not missing"

            if test "${#EXPLICITLY_REQUESTED_REPOS[@]}" -gt 0; then

                # If we are falling through here then it means that the user
                # explicitly requested the repo, but it's working directory is
                # already present. This will ultimately result in the program
                # exiting with an error status because we are not adding the
                # repo name to our $PROCESSED_REPOS array. With that being the
                # case, we'll print the following message as a warning.

                printf "${PROG} (warning): subdirectory exists for repo name \"%s\"; --missing-only specified; skipping\n" \
                       "${repo_name}" 1>&2
            else

                if $BE_VERBOSE; then
                    printf "${PROG} (info): subdirectory exists for repo name \"%s\"; --missing-only specified; skipping (okay)\n" \
                           "${repo_name}" 1>&2
                fi
            fi

            continue
        fi
    fi

    # If there's a subdirectory beneath the current location with the
    # repository name, then it is probably the git working directory for the
    # project. Note that it is possible to have a project cloned into a
    # directory that does not exactly match the project name, but I don't do
    # that with my own repos.
    #
    if test -d "${repo_name}"; then :; else

        if $DO_CLONE_IF_NOT_PRESENT; then

            if $BE_VERBOSE; then
                if $FETCH_ONLY_MISSING; then
                    # The '--missing-only' option implies '--clone-if-missing',
                    # so we show a slightly different log message to help avoid
                    # confusion in case the user did not explicitly provide
                    # '--clone-if-missing' (which would be redundant if '--missing-only'
                    # was provided).
                    #
                    printf "${PROG} (info): no subdirectory exists for repo name \"%s\"; --missing-only specified, so will clone repo from GitHub\n" "${repo_name}" 1>&2
                else
                    printf "${PROG} (info): no subdirectory exists for repo name \"%s\"; --clone-if-missing specified, so will clone repo from GitHub\n" "${repo_name}" 1>&2
                fi
            fi

            declare t_v_opts=()
            if $BE_VERBOSE; then t_v_opts+=( '--verbose' ); fi
            if $DEBUGGING;  then t_v_opts+=( '--verbose' ); fi

            declare t_v_clone_opts=()

            # GitHub issue #38: ads-github-fetch-all-upstreams: chokes attempting to clone non-fork repos
            #
            # When cloning a repo, the default behavior of git-hub(1) is to
            # create a triangular workflow configuration, which is usually
            # what you want when there is and upstream repo from which your
            # origin repo was forked. However, if the repo is not a GitHub
            # fork of some other GitHub repo, then we need to tell git-hub(1)
            # to not try to create a triangular workflow config; attempting to
            # do so will result in the error:
            #
            #     Warning: Repository username/some-repo is not a fork, just cloning, upstream will not be set
            #     usage: git-hub [-h] [--version] [-v] [-s] {clone,issue,pull,setup} ...
            #     git-hub: error: Can't use triangular workflow without an upstream repo
            #     ads-github-fetch-all-upstreams (error): was error while attempting to clone repo "some-repo" from GitHub; bailing out
            #
            # See: https://github.com/salewski/ads-github-tools/issues/38
            #
            if $is_fork; then :; else
                t_v_clone_opts+=( '--no-triangular' );
            fi

            "${GIT_PROG}" hub "${t_v_opts[@]}" clone "${t_v_clone_opts[@]}" "${repo_name}"

            if test $? -ne 0; then
                printf "${PROG} (error): was error while attempting to clone repo \"%s\" from GitHub; bailing out\n" "${repo_name}" 1>&2
                exit 1
            fi

            # Sanity check
            if test -d "${repo_name}"; then :; else
                printf "${PROG} (error): just cloned repo \"%s\", but subdir with that name is not present; bailing out\n" "${repo_name}" 1>&2
                exit 1
            fi

            unset t_v_clone_opts
            unset t_v_opts
        else
            if $DEBUGGING; then
                printf "${PROG} (debug): no subdirectory named after repo \"%s\"; skipping (okay)\n" "${repo_name}" 1>&2
            fi

            continue
        fi
    fi

    if test -d "${repo_name}/.git"; then :; else
        printf "${PROG} (warning): subdirectory named after repo \"%s\" exists, but is not a git working directory; skipping\n" "${repo_name}" 1>&2
        continue
    fi

    # The main purpose of this program is to 'git fetch upstream', but we know
    # that a repo that is not a fork does not have an 'upstream'.
    #
    # XXX: Note that we're doing this check here (rather than at the top of
    # the loop) to allow for the side effect of cloning GitHub repos if there
    # is no working directory present. This behavior should be considered
    # experimental and subject to change in the future. It happens to be
    # convenient for me to do it this way at the moment, but my gut says we
    # might be better off having a separate tool whose job it is to perform
    # just that task.
    #
    # XXX: You may have other remotes defined, or have a non-github repo that
    # has a remote named 'upstream', but this tool will not operate on
    # them. Our purpose here is only to fetch the 'upstream' repo for GitHub
    # repos.
    #
    if $is_fork; then :; else
        if $BE_VERBOSE; then
            printf "${PROG} (info): repo \"%s\" is not a fork of another GitHub repo, so we do not need to check for an \"upstream\" remote; skipping (okay)\n" "${repo_name}" 1>&2
        fi

        # We will treat this as "processed", just with nothing to do. There's
        # no reason to error out when processing explicitly specified repos
        # just because one or more of them are not forks of other GitHub
        # repos.
        #
        PROCESSED_REPOS+=( "${repo_name}" )

        continue
    fi

    unset CDPATH
    (
        set -o pipefail  # not inherited from parent shell

        cd "${repo_name}" || exit 1  # from subshell

        declare -a t_pipestatus_hold

        "${GIT_PROG}" remote | grep -q --max-count=1 '^upstream$'

        t_pipestatus_hold=( ${PIPESTATUS[@]} )

        if test "${#t_pipestatus_hold[@]}" -ne 2; then
            # This is only possible if we've broken something above, which I
            # did do during development, so I'm leaving it in.
            printf "${PROG} (error): was error while attempting to determine if repo \"%s\" has an \"upstream\" remote defined; bailing out\n" "${repo_name}" 1>&2
            exit 1
        fi

        if test "${t_pipestatus_hold[0]}" -eq 0 \
        && test "${t_pipestatus_hold[1]}" -eq 0; then

            if $DEBUGGING; then
                printf "${PROG} (debug): working dir for repo \"%s\" has an \"upstream\" remote defined\n" "${repo_name}" 1>&2
            fi
            # Keep going...

        elif test "${t_pipestatus_hold[0]}" -eq 0 \
          && test "${t_pipestatus_hold[1]}" -eq 1; then

            # No 'upstream' remote is defined for this repo. Either it is the
            # user's own project (not a fork of some other project), or the
            # 'upstream' remote just wasn't set up. It should be the latter
            # case, though, because we checked the 'fork' flag from the repo's
            # GitHub metadata.

            if $BE_VERBOSE; then
                printf "${PROG} (info): did not find an \"upstream\" remote configured for repo \"%s\"\n" "${repo_name}" 1>&2
            fi

            if $DO_CREATE_UPSTREAM_REMOTE_IF_NOT_PRESENT; then

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

                if $BE_VERBOSE; then
                    printf "${PROG} (info): '-u' (--upstream-remote-if-missing) option was specified; will attempt to create \"upstream\" git remote for repo \"%s\"\n" "${repo_name}" 1>&2
                fi

                # We will set up (using the git-hub tool) an "upstream" remote
                # that points to the parent.

# FIXME: go
#     I thought git-hub would just add the 'upstream' repo here, but upon
#     closer reading I see that is not the case. We're just doing this inline
#     for now, but it might be a better fit as an enhancement to 'git-hub'.
                # printf "${PROG} (WIP): '--upstream-remote-if-missing' feature not yet fully implemented; bailing out\n" 1>&2
                # exit 1
# FIXME: end

                # declare t_v_opts2=()
                # if $BE_VERBOSE; then t_v_opts2+=( '--verbose' ); fi
                # if $DEBUGGING;  then t_v_opts2+=( '--verbose' ); fi

                # git hub "${t_v_opts2[@]}" clone "${repo_name}"

                # if test $? -ne 0; then
                #     printf "${PROG} (error): was error while attempting to establish the \"upstream\" remote for repo \"%s\" from via 'git hub clone...'; bailing out\n" "${repo_name}" 1>&2
                #     exit 1
                # fi


                # We'll ask the GitHub service if this repo has a "parent",
                # which it should (read as: "will, modulo changes due to
                # timing since we checked") because we've already checked
                # above. If not (won't happen), then there's nothing left to
                # do. Note that we have to make a call to the service
                # inquiring about a specific repo to obtain the parent's
                # 'clone_url'; it is not available in the output of the full
                # list of repos which we've already obtained. This could be a
                # bit expensive against your rate limit if you have a large
                # number of repos, but if most of those repos are forks it
                # will only be expensive the first time the program is run to
                # update those repos.

                # From the GitHub v3 API:
                #
                #     Get
                #
                #     GET /repos/:owner/:repo
                #     Response
                #
                #     The parent and source objects are present when the
                #     repository is a fork. parent is the repository this
                #     repository was forked from, source is the ultimate source
                #     for the network.

                # Note that the repo_full_name value here has the form
                # 'username/repo_name', so matches the structure of the URL
                # needed.
                #
                t_gh_url_for_this_repo="${gl_const_github_api_base_url}repos/${repo_full_name}"

                if $BE_VERBOSE; then
                    printf "${PROG} (info): querying GitHub service to obtain parent clone url for repo \"%s\"\n" "${repo_name}" 1>&2
                fi

                MY_CURL_OPTS=()  # (re)set
                MY_CURL_OPTS+=( "${MY_CURL_DEFAULT_OPTS[@]}" )  # copy

                MY_CURL_OPTS+=( '--get' )

                t_http_code=$( "${CURL_PROG}" "${MY_CURL_OPTS[@]}" \
                                              "${t_gh_url_for_this_repo}")
                t_estat=$?
                if test ${t_estat} -ne 0; then
                    printf "${PROG} (error): was error while invoking curl(1) to retrieve repo \"%s\"; bailing out\n" \
                           "${repo_full_name}" 1>&2
                    exit 1
                fi
                if test -z "${t_http_code}"; then
                    printf "${PROG} (error): ${FUNCNAME}(): curl(1) command succeeded, but no HTTP response code emitted; bailing out\n" 1>&2
                    exit 1
                fi
                case ${t_http_code} in
                    '200')
                        if $DEBUGGING; then
                            printf "${PROG} (debug): GET request for repo \"%s\" succeeded\n" "${repo_full_name}" 1>&2
                        fi
                        ;;
                    *)
                        printf "${PROG} (error): GET request for repo \"%s\" failed; HTTP response code was: \"%s\"; expected 200 (\"OK\"); bailing out\n" \
                               "${repo_full_name}" \
                               "${t_http_code}" 1>&2
                        exit 1
                        ;;
                esac

                if test -s "${MY_TMP_CURL_OUT_FPATH}"; then :; else
                    printf "${PROG} (error): GET request for repo \"%s\" succeeded, by response body is empty; bailing out\n" \
                           "${repo_full_name}" 1>&2
                    exit 1
                fi

                t_parent_clone_url=$("${JQ_PROG}" --raw-output '.clone_url'  \
                                                  < "${MY_TMP_CURL_OUT_FPATH}")
                if test $? -ne 0; then
                    printf "${PROG} (error): was unable to extract clone url for repo \"%s\"; bailing out\n" "${repo_name}" 1>&2
                    exit 1  # from subshell
                fi
                if test -z "${t_parent_clone_url}"; then
                    printf "${PROG} (error): was unable to obtain clone url for repo \"%s\"; bailing out\n" "${repo_name}" 1>&2
                    exit 1  # from subshell
                fi

                if $BE_VERBOSE; then
                    printf "${PROG} (info): setting up \"upstream\" remote for repo \"%s\" to: %s\n" "${repo_name}" "${t_parent_clone_url}" 1>&2
                fi

                "${GIT_PROG}" remote add upstream "${t_parent_clone_url}"

                if test $? -ne 0; then
                    printf "${PROG} (error): was unable to establish \"upstream\" remote for repo \"%s\"; bailing out\n" "${repo_name}" 1>&2
                    exit 1
                fi

                if $DEBUGGING; then
                    printf "${PROG} (info): successfully set up \"upstream\" remote for repo \"%s\" to: %s\n" "${repo_name}" "${t_parent_clone_url}" 1>&2
                fi

# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            else
                printf "${PROG} (warning): no remote named \"upstream\" defined for repo \"%s\"; skipping (okay)\n" "${repo_name}" 1>&2
                exit 0  # from subshell
            fi

        else

# FIXME: Introduce a "keep going" feature to avoid bailing out here
# FIXME: Introduce a "skip repo" feature to allow caller to exclude processing of some repo subidrs

            printf "${PROG} (error): was error while attempting to determine if repo \"%s\" has an \"upstream\" remote defined; bailing out\n" "${repo_name}" 1>&2
            exit 1
        fi

        if $BE_VERBOSE; then
            printf "${PROG} (info): invoking 'git fetch upstream' for repo \"%s\"\n" "${repo_name}" 1>&2
        fi

        "${GIT_PROG}" fetch upstream
        if test $? -ne 0; then
            printf "${PROG} (error): was error while attempting to 'git fetch upstream' for repo \"%s\"; bailing out\n" "${repo_name}" 1>&2
            exit 1
        fi

        if $DEBUGGING; then
            printf "${PROG} (debug): successfully invoked 'git fetch upstream' for repo \"%s\"\n" "${repo_name}" 1>&2
        fi
    )

    if test $? -ne 0; then
# FIXME: see above note about introducing a "keep going" feature

        # We're expecting that an error message would have already been printed by the subshell
        exit $?
    fi

    PROCESSED_REPOS+=( "${repo_name}" )

done < <(echo "$REPO_DATA_LINES")


if test ${#EXPLICITLY_REQUESTED_REPOS[@]} -gt 0; then

    # We use this to "remember" the names of all repo names that have not been
    # processed. This allows us to present the complete list to the user in
    # our error message.
    #
    # Using an assoc. array to automatically dedup the list of names. Each key
    # is the name of a repo that should appear in the error message.
    #
    # See also: https://github.com/salewski/ads-github-tools/issues/23
    #
    declare -A t_unprocessed_repo_names_to_counts=()

    for t_requested_repo_name in "${EXPLICITLY_REQUESTED_REPOS[@]}"; do

        t_processed_it=false

        for t_processed_repo_name in "${PROCESSED_REPOS[@]}"; do
            if test "${t_requested_repo_name}" = "${t_processed_repo_name}"; then
                t_processed_it=true
                break
            fi
        done

        if $t_processed_it; then :; else
            (( ++t_unprocessed_repo_names_to_counts["${t_requested_repo_name}"] ))
            : $PROG \(trace: $LINENO\): name of unprocessed repo name \""${t_requested_repo_name}"\" seen ${t_unprocessed_repo_names_to_counts["${t_requested_repo_name}"]} time\(s\)
        fi
    done

    if test "${#t_unprocessed_repo_names_to_counts[@]}" -gt 0; then
        t_maybe_ess='s'
        if test "${#t_unprocessed_repo_names_to_counts[@]}" -eq 1; then
            t_maybe_ess=''
        fi

        # Need single value for our printf '%s' conversion specification
        # below.
        #
        # Feeding the names of all unprocessed repo names (one per line) to
        # 'xargs echo' has the effect of flattening the list down to a single
        # line (well, a single line for each 'echo' invocation; in the common
        # case all will fit on a single command line an result in the pretty
        # output that we intend, but this will work either way because we
        # capture the output as a single value -- if multiple lines are
        # produced the output will just be a bit uglier).
        #
        # Note that (non-built-in) 'echo' is the default "utility" invoked by
        # xargs since at least as far back as SUSv2 (1998), but we specify it
        # explicitly for clarity.
        #
        t_unprocessed_repo_names_as_string="$( printf '%s\n' "${!t_unprocessed_repo_names_to_counts[@]}" | "${XARGS_PROG}" "${ECHO_PROG}" )"
        if test $? -ne 0; then
            printf "${PROG} (error): was unable to flatten list of %d unprocessed repo names; bailing out\n" \
                   "${#t_unprocessed_repo_names_to_counts[@]}" 1>&2
            exit 1
        fi

        printf "${PROG} (error): did not process explicitly requested repo%s: %s; bailing out\n" \
               "${t_maybe_ess}" \
               "${t_unprocessed_repo_names_as_string}" 1>&2
        exit 1
    fi
fi

if $BE_VERBOSE; then
    printf "${PROG} (info): completed successfully\n" 1>&2
fi

exit 0;


#
# ----------------------------------------------------------------------------
# Documentation
#
# The docs are in Perl's POD format, so you can run either the 'perldoc' or
# 'pod2man' programs on this file to produce a man page.
#
# To generate a man page for distribution (in a tarball or RPM, for instance),
# you'll probably want to run pod2man something like this:
#
#     $ pod2man /path/to/this/file \
#               --center='ads-github-tools' \
#               --release='ads-github-tools-0.1.0' \
#               --section='1' \
#               > /outputdir/ads-github-fetch-all-upstreams.1
#
# To inspect the page formatting, etc., you can pipe the above 'pod2man'
# command to:
#
#     'man -l -'
#
# instead of redirecting the output to a file.
#
# ----------------------------------------------------------------------------

=pod

=head1 NAME

  ads-github-fetch-all-upstreams - Fetch all of user's GitHub repos, if needed


=head1 SYNOPSIS

  ads-github-fetch-all-upstreams --help
  ads-github-fetch-all-upstreams --version

  ads-github-fetch-all-upstreams [OPTION...] [REPO...]


=head1 DESCRIPTION

The C<ads-github-fetch-all-upstreams> program is part of the
C<ads-github-tools> project.

The C<ads-github-fetch-all-upstreams> program invokes C<git fetch upstream>
for all of the user's GitHub repos that are present (in repository working
directories beneath the current location). With appropriate options, will
clone some or all missing repos and set up the 'upstream' remote.

FIXME: docs still a wip


=head1 OPTIONS

Below are the command line options currently accepted by
C<ads-github-fetch-all-upstreams>.


=over 4

=item -h, --help

Print help usage message


=item -V, --version

Print the version of the program to stdout and then exit.


=item -c, --clone-if-missing

Use L<git-hub(1)> to clone your missing repos from GitHub.


=item -m, --missing-only

Fetch only repos that are missing locally.

The default behavior of the program is to operate only on working directories
that are present, or only on those repos that are explicitly named on the
command line (two different flavors of "operate only on these" semantics).

Sometimes you want exactly the opposite behavior, though ("fetch all the new
stuff" semantics). Our C<--missing-only> command line option allows the user
to request that behavior.

This option implies C<--clone-if-missing>) (C<-c>), and effectively provides a
shorthand way of invoking:

    $ ads-github-fetch-all-upstreams -c <MISSING_REPO>...

without having to explicitly name (or even know) the values for each missing
repository name.

This behavior is useful, for example, if you have forked a number of different
GitHub repos and want to have them cloned locally without having to wait for
the status of all existing repositories to be checked (which can be a long
time if you have a large number of GitHub repos), and without having to
explicitly name the missing repos on the command line.


=item -u, --upstream-remote-if-missing

Setup 'upstream' remote in existing repo working dirs, if missing


=item -v, --verbose

Turn on verbose mode. Causes program to emit messages on C<stderr> indicating
what it is doing. The option may be specified multiple times to increase
further the level of verbosity. One C<-v> option enables info-level output;
two such opts enable debug-level output; three or more enable trace-level
output.


=item --

Signals the end of options and disables further options processing. This is
useful in the pathological scenario in which a REPO parameter would otherwise
be interpreted as a command line option.

=back


=head1 DIAGNOSTICS

Exits with zero on success, non-zero on error.


=head1 BUGS

=over 4

=item Results from calls to the GitHub API are not currently cached

At the time of writing (2016-05) the author is still working on the
'ads-github-tools' cache implementation, so any information needed from the
upstream GitHub service is requested directly. This may incur unnecessary hits
against the user's GitHub API rate limit. Don't worry, in the typical case we
are only talking about a handful of requests even if the user has a very large
number of repositories (i.e., several hundred).

=item Probably others

If you find any, please report them as described in the C<BUGS> file.

=back


=head1 SEE ALSO

=over 4

=item ads-github-tools(7)

=item ads-github-merge-all-upstreams(1)

=back


=head1 AUTHOR

=over 4

=item Alan D. Salewski  <ads@salewski.email>

=back


=head1 COPYRIGHT

Copyright 2016, 2017, 2019, 2020 Alan D. Salewski

=cut

